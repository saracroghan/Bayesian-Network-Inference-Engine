{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUeNWRBtbpna"
      },
      "outputs": [],
      "source": [
        "# PARAMETERS\n",
        "GROUP_ID = 'Group11'\n",
        "ALGORITHM = 've'\n",
        "NETWORK_NAME = \"/content/drive/MyDrive//networks/child.bif\"\n",
        "REPORT = 'Disease'\n",
        "EVIDENCE_LEVEL = 'None'\n",
        "EVIDENCE = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5w5h47dnJRD",
        "outputId": "f0b2f58f-25e4-45c3-ad18-ef266225d000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive, unneccessary if not using google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l7woXKHol-p"
      },
      "outputs": [],
      "source": [
        "# CLASSES\n",
        "# imports\n",
        "import itertools\n",
        "\n",
        "# an object to represent a factor in a probabilistic model\n",
        "# like a table over a set of vars and probs\n",
        "# equivalent to a conditional prob table (cpt)\n",
        "# variables = list of Node objects\n",
        "# proabilities = dict matching values to their probs\n",
        "class Factor:\n",
        "    def __init__(self, variables, probabilities):\n",
        "        self.variables = variables\n",
        "        self.probabilities = probabilities\n",
        "        self.variable_names = [v.name for v in variables]\n",
        "\n",
        "    def get_var_scope(self): # get all names of variables associated with the factor\n",
        "        return self.variable_names\n",
        "\n",
        "    def __str__(self):# to print factors\n",
        "      header = \" | \".join(self.variable_names + [\"Probability\"])\n",
        "      separator = \"-\" * len(header)\n",
        "      rows = [header, separator]\n",
        "      for assignment, prob in self.probabilities.items():\n",
        "        row_str = \" | \".join(map(str, assignment)) + f\" | {prob:.4f}\"\n",
        "        rows.append(row_str)\n",
        "      return \"\\n\".join(rows)\n",
        "\n",
        "    def multiply_factors(factor1, factor2): # multiple two factor objects via the use of the cartesian product\n",
        "      vars1= factor1.variable_names\n",
        "      vars2= factor2.variable_names\n",
        "      new_vars_names = sorted(list(set(vars1)|set(vars2))) # get union of variable for new factor\n",
        "\n",
        "      # map var names to associated nodes\n",
        "      var_map = {v.name: v for v in factor1.variables}\n",
        "      var_map.update({v.name: v for v in factor2.variables})\n",
        "      new_vars_nodes = [var_map[name] for name in new_vars_names]\n",
        "\n",
        "      new_probabilities= {}\n",
        "\n",
        "      # use cartesian product to generate every row for the new factor table\n",
        "      domains = [v.domain for v in new_vars_nodes]\n",
        "      for row in cartesian_product(domains):\n",
        "        full_row_dict = dict(zip(new_vars_names, row))\n",
        "        row1_tuple = tuple(full_row_dict[v_name] for v_name in vars1)\n",
        "        row2_tuple = tuple(full_row_dict[v_name] for v_name in vars2)\n",
        "\n",
        "        prob1 = factor1.probabilities.get(row1_tuple, 1.0)\n",
        "        prob2 = factor2.probabilities.get(row2_tuple, 1.0)\n",
        "\n",
        "        new_probabilities[row] = prob1 * prob2\n",
        "      return Factor(new_vars_nodes, new_probabilities)\n",
        "\n",
        "    def marginalize(factor, var_to_sum_out): # sum out a variable from a factor with marginilization\n",
        "      if var_to_sum_out not in factor.variable_names:\n",
        "        return factor # there is no var to sum out in this factor//doesn't exist\n",
        "\n",
        "      new_vars_nodes  = [v for v in factor.variables if v.name != var_to_sum_out]\n",
        "      sum_out_index = factor.variable_names.index(var_to_sum_out)\n",
        "\n",
        "      new_probabilities = {}\n",
        "      for row, prob in factor.probabilities.items():\n",
        "        new_row_tuple = tuple(val for i, val in enumerate(row) if i != sum_out_index)\n",
        "        new_probabilities[new_row_tuple] = new_probabilities.get(new_row_tuple, 0.0) + prob\n",
        "\n",
        "      return Factor(new_vars_nodes, new_probabilities)\n",
        "\n",
        "    def get_prob_from_dict(self, assignment_dict):\n",
        "      # looks up prob for a specific assignment\n",
        "\n",
        "      # build a tuple in the order that matches how the factor's prob dict is\n",
        "      assignment_tuple = tuple(assignment_dict[var_name] for var_name in self.variable_names)\n",
        "      # return the prob\n",
        "      return self.probabilities.get(assignment_tuple, 0.0)\n",
        "\n",
        "class Node:\n",
        "  # node object to hold its name, domain, parents, children, and releavant factor/conditional prob table\n",
        "  def __init__(self, name, domain):\n",
        "    self.name = name\n",
        "    self.domain = domain\n",
        "    self.parents = []\n",
        "    self.children = []\n",
        "    self.cpt = None # factor object\n",
        "\n",
        "  def set_cpt(self, probabilities): #conditional probability table for the node using factor class\n",
        "    cpt_vars = self.parents + [self]\n",
        "    factor_probs = {}\n",
        "\n",
        "    # parent domains to generate all state combos\n",
        "    parent_domains = [parent.domain for parent in self.parents]\n",
        "\n",
        "    # generate all parent state combos\n",
        "    parent_rows = cartesian_product(parent_domains) if parent_domains else [()]\n",
        "\n",
        "    # go through each combo of parent rows\n",
        "    for i, p_row in enumerate(parent_rows):\n",
        "      # get probs of the childs states given this parent row\n",
        "      child_probs_list = probabilities[p_row]\n",
        "\n",
        "      #match each child state with its prob\n",
        "      for j, child_state in enumerate(self.domain):\n",
        "        # include parents values and childs values\n",
        "        full_row = p_row + (child_state,)\n",
        "        factor_probs[full_row] = child_probs_list[j]\n",
        "\n",
        "\n",
        "    self.cpt = Factor(cpt_vars, factor_probs)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Node({self.name})\"\n",
        "\n",
        "class BayesNet:\n",
        "  # an object to represent a bayes net, holds all nodes in the net and edges between them\n",
        "  def __init__(self):\n",
        "    self.nodes = {} # to map node names to node objects\n",
        "\n",
        "  def add_node(self, node):\n",
        "    if node.name in self.nodes:\n",
        "      raise ValueError(f\"Node with name {node.name} already exists\")\n",
        "    self.nodes[node.name] = node\n",
        "\n",
        "  def add_edge(self, parent_name, child_name):\n",
        "    if parent_name not in self.nodes or child_name not in self.nodes:\n",
        "      raise ValueError(\"both parents and child must be in network to add edge\")\n",
        "    parent_node = self.nodes[parent_name]\n",
        "    child_node = self.nodes[child_name]\n",
        "\n",
        "    parent_node.children.append(child_node)\n",
        "    child_node.parents.append(parent_node)\n",
        "\n",
        "  def get_node(self, name):\n",
        "    return self.nodes.get(name)\n",
        "\n",
        "def cartesian_product(domains):\n",
        "  # get cartesian product of a list of lists\n",
        "  # returns list of tuples with all combo tuples\n",
        "  # get every combo of the domains\n",
        "\n",
        "  if not domains:\n",
        "    return [()] # base case\n",
        "\n",
        "  # recursion\n",
        "  results = []\n",
        "  first_domain = domains[0]\n",
        "  other_domains = domains[1:]\n",
        "\n",
        "  # get all combos for other domains\n",
        "  combos_of_other_domains = cartesian_product(other_domains)\n",
        "\n",
        "  # for first domain, combine it with each combo from the other domains\n",
        "  for value in first_domain:\n",
        "    for combo in combos_of_other_domains:\n",
        "      results.append((value,)+combo)\n",
        "\n",
        "  return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usebhHjon6tn",
        "outputId": "7df09dfc-db21-42de-bfa6-bd25cbd8f002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pgmpy\n",
            "  Downloading pgmpy-1.0.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pgmpy) (3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pgmpy) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pgmpy) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from pgmpy) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from pgmpy) (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from pgmpy) (2.8.0+cu126)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (from pgmpy) (0.14.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from pgmpy) (4.67.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from pgmpy) (1.5.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.12/dist-packages (from pgmpy) (3.4.0)\n",
            "Collecting pyro-ppl (from pgmpy)\n",
            "  Downloading pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->pgmpy) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->pgmpy) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->pgmpy) (2025.2)\n",
            "Collecting pyro-api>=0.1.1 (from pyro-ppl->pgmpy)\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->pgmpy) (3.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pgmpy) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels->pgmpy) (1.0.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels->pgmpy) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->pgmpy) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->pgmpy) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->pgmpy) (3.0.3)\n",
            "Downloading pgmpy-1.0.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pyro-api, pyro-ppl, pgmpy\n",
            "Successfully installed pgmpy-1.0.0 pyro-api-0.1.2 pyro-ppl-1.9.1\n"
          ]
        }
      ],
      "source": [
        "# BIF Import\n",
        "\n",
        "!pip install pgmpy #- to install pgmpy\n",
        "from pgmpy.readwrite import BIFReader\n",
        "\n",
        "def load_bif(file_path):\n",
        "  # Load BIF\n",
        "  reader = BIFReader(file_path)\n",
        "  model = reader.get_model()\n",
        "\n",
        "  # Extract BIF information\n",
        "  variables = reader.get_variables()\n",
        "  parents = reader.get_parents()\n",
        "  states = reader.get_states()\n",
        "  cpds = {cpd.variable: cpd for cpd in model.get_cpds()}\n",
        "\n",
        "  # Create Bayesian network and add nodes\n",
        "  bayes_net = BayesNet()\n",
        "\n",
        "  # Create nodes\n",
        "  for var in variables:\n",
        "    node = Node(name=var, domain=states[var])\n",
        "    bayes_net.add_node(node)\n",
        "\n",
        "  # Add parent child edges\n",
        "  for child, parent_list in parents.items():\n",
        "    for parent in parent_list:\n",
        "      bayes_net.add_edge(parent, child)\n",
        "\n",
        "  # CPTs for each node\n",
        "  for var in variables:\n",
        "    node = bayes_net.get_node(var)\n",
        "    cpd = cpds[var]\n",
        "\n",
        "    parent_names_cpd_order = cpd.variables[1:]\n",
        "\n",
        "    parent_domain_lists = [states[p_name] for p_name in parent_names_cpd_order]\n",
        "    child_domain_list = node.domain\n",
        "\n",
        "    probabilities = {}\n",
        "\n",
        "    # Create every possible combo of parent states\n",
        "    parent_rows = cartesian_product(parent_domain_lists) if parent_domain_lists else[()]\n",
        "\n",
        "    #get numpy array of probabilites\n",
        "    cpd_vals_array = cpd.values\n",
        "\n",
        "    # Assign probability of each child state from CPD\n",
        "    for p_row in parent_rows:\n",
        "      if parent_domain_lists:\n",
        "        p_indicies = tuple(parent_domain_lists[i].index(p_row[i]) for i in range(len(p_row)))\n",
        "        for j, child_val in enumerate(child_domain_list):\n",
        "          # full index of array\n",
        "          cpt_index = (j,) + p_indicies\n",
        "\n",
        "          # get key for factors dict\n",
        "          row = p_row + (child_val,)\n",
        "          probabilities[row] = float(cpd_vals_array[cpt_index])\n",
        "      else:\n",
        "        for j, child_val in enumerate(child_domain_list):\n",
        "          row = p_row + (child_val,)\n",
        "          probabilities[row] = float(cpd.values[j])\n",
        "\n",
        "    parent_nodes_in_order = [bayes_net.get_node(p_name) for p_name in parent_names_cpd_order]\n",
        "    # Create factor then assign to the node\n",
        "    node.cpt = Factor(parent_nodes_in_order + [node], probabilities)\n",
        "\n",
        "  return bayes_net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJA00otU0Pr0"
      },
      "outputs": [],
      "source": [
        "# GIBBS SAMPLING\n",
        "import random\n",
        "\n",
        "def gibbs(bayes_net, query, evidence, num_samples, burn_num):\n",
        "  # input: bayes_net: bayes net object\n",
        "  # query: name of variable to we are trying to find out about\n",
        "  # evidence: dictionary for observed evidence\n",
        "  # num_sample: number of samples to do\n",
        "  # burn_num: number of intial samples to discard\n",
        "  # output: prob distribution dictionary of query var\n",
        "\n",
        "  # initialize the dist\n",
        "  current_state = forward_sampling(bayes_net, evidence)\n",
        "  # keep track of nodes we don't have evidence for\n",
        "  no_evidence_nodes = [\n",
        "      node for name, node in bayes_net.nodes.items()\n",
        "      if name not in evidence]\n",
        "  # get query node object\n",
        "  query_node = bayes_net.get_node(query)\n",
        "  counts = {value: 0 for value in query_node.domain} #initialize tally for how often a possible value occurs for each query var\n",
        "\n",
        "  # run burn in\n",
        "  for i in range(burn_num):\n",
        "    for node in no_evidence_nodes:\n",
        "      new_val = sample_markov_blanket(node, current_state, bayes_net)\n",
        "      current_state[node.name] = new_val\n",
        "\n",
        "  # after burn in\n",
        "  for i in range(num_samples):\n",
        "    for node in no_evidence_nodes:\n",
        "      new_val = sample_markov_blanket(node, current_state, bayes_net)\n",
        "      current_state[node.name]= new_val\n",
        "    # accumulate result for loop\n",
        "    query_val = current_state[query]\n",
        "    counts[query_val] += 1 # counts??\n",
        "  return normalize(counts)\n",
        "\n",
        "def topological_sort(bayes_net):\n",
        "  # perform topological sort on bayes net\n",
        "  # input: bayes net\n",
        "  # output: list of nodes in topological order\n",
        "\n",
        "  # find number of parents for each node\n",
        "  num_parents = {name: len(node.parents) for name, node in bayes_net.nodes.items()}\n",
        "\n",
        "  # queue all nodes with no parents\n",
        "  queue = [name for name, count in num_parents.items() if count == 0]\n",
        "\n",
        "  sorted_nodes = []\n",
        "  # go through all nodes in the queue until there are no more\n",
        "  while queue:\n",
        "    # pop each node on the queue and append them because they are part of the current \"layer\"\n",
        "    node_name = queue.pop(0)\n",
        "    node = bayes_net.get_node(node_name)\n",
        "    sorted_nodes.append(node)\n",
        "\n",
        "    # remove parent from each child so that they can be queued\n",
        "    for child in node.children:\n",
        "      num_parents[child.name] -= 1\n",
        "      # if all parents are removed for a child, add them to the queue because we are in the next layer down\n",
        "      if num_parents[child.name] == 0:\n",
        "        queue.append(child.name)\n",
        "  return sorted_nodes\n",
        "\n",
        "\n",
        "def forward_sampling(bayes_net, evidence):\n",
        "  # initialize first state for gibbs sampling using forward sampling\n",
        "  # input: bayes net: bayes net object\n",
        "  # evidence: dictionary of given evidence\n",
        "  # output: dictionary with prob dist for each node by name\n",
        "  current_state = {}\n",
        "  # do topological sort of given bayes net\n",
        "  sorted_nodes = topological_sort(bayes_net)\n",
        "  for node in sorted_nodes:\n",
        "    # if node has evidence, make its value fixed\n",
        "    if node.name in evidence:\n",
        "      current_state[node.name] = evidence[node.name]\n",
        "    else: # non evidence node, sample it, get prob dist given the parents from current state\n",
        "      prob_given_parents = get_prob(node, current_state)\n",
        "      # sample value from the above dist\n",
        "      current_state[node.name] = weighted_sample(prob_given_parents)\n",
        "  return current_state\n",
        "\n",
        "def get_prob(node, current_state):\n",
        "  # get the probability of a node given its parents from its conditional prob table\n",
        "  # input: node: node object for which we are trying to get the prob of\n",
        "  # current_state: current state dict holding prob dist for each node by name\n",
        "  # output: prob dist dictionary for a child given its parents values\n",
        "  # get parent values\n",
        "  parents = node.cpt.variable_names[:-1]\n",
        "\n",
        "  # if root node (no parents)\n",
        "  if not parents:\n",
        "    parent_assignment = ()\n",
        "  else:\n",
        "    parent_assignment = tuple(current_state[name] for name in parents)\n",
        "\n",
        "  # get probs for these parent assignments\n",
        "  prob_dict = {}\n",
        "  for child_val in node.domain:\n",
        "    full_assignment = parent_assignment + (child_val,)\n",
        "    # assume node.cpt.proabilites is the dict in the node object\n",
        "    prob_dict[child_val] = node.cpt.probabilities[full_assignment]\n",
        "\n",
        "  return prob_dict\n",
        "\n",
        "def sample_markov_blanket(node, current_state, bayes_net):\n",
        "  # input: node: node object, current_state: current state dict holding prob dist for each node by name,\n",
        "  # bayes_net, bayes net object\n",
        "  # output: value sample, given markov blanket from prob dict randomly, based on its weight\n",
        "  # calculate the probability of a node given the markov blanket for that node\n",
        "  probs = {}\n",
        "  temp_state = current_state.copy() # create copy of current state\n",
        "  # calculate unnormalized prob for each possible val of the var/node given\n",
        "  for value in node.domain:\n",
        "    # set the nodes value in the current state dict so that we can calculate other probs based on that\n",
        "    temp_state[node.name] = value\n",
        "    prob = node.cpt.get_prob_from_dict(temp_state)\n",
        "    # compute product\n",
        "    for child in node.children:\n",
        "      # get prob of child given its parents (including the node)\n",
        "      prob *= child.cpt.get_prob_from_dict(temp_state)\n",
        "    probs[value] = prob\n",
        "  # normalize the prob dict\n",
        "  normalized_probs = normalize(probs)\n",
        "  # handle deterministic case\n",
        "  if normalized_probs is None:\n",
        "    return current_state[node.name] # reject change and return nodes current val\n",
        "  else:\n",
        "  # return a single sample from the calculated dist\n",
        "    return weighted_sample(normalized_probs)\n",
        "\n",
        "def normalize(prob_dict):\n",
        "  # convert a dictionary of that maps a value to a number and normalize it so all numbers in it sum to 1\n",
        "  # input: proability dictionary\n",
        "  # output: normalized probability dictionary\n",
        "  total = sum(prob_dict.values())\n",
        "  # if all probs are zero make it a uniform dist\n",
        "  if total == 0:\n",
        "    return None\n",
        "  else:\n",
        "    return {value: prob/total for value, prob in prob_dict.items()}\n",
        "\n",
        "def weighted_sample(prob_dict):\n",
        "# take normalized dict of probs and returns single value sampled according to these probs\n",
        "# the sampling part to be used in gibbs\n",
        "  # input: prob dictionary\n",
        "  # output: value sample from prob dict randomly, based on its weight\n",
        "\n",
        "  # get random num between 1 and 0\n",
        "  rand_val = random.random()\n",
        "  # initialize a cum prob to keep track of probs we've seen\n",
        "  cumulative_prob = 0.0\n",
        "  # go through each item in dict\n",
        "  for value, prob in prob_dict.items():\n",
        "    # add each prob to our running total\n",
        "    cumulative_prob += prob\n",
        "    # check if random number falls within the prob (is less than)\n",
        "    if rand_val < cumulative_prob:\n",
        "      # if yes than choose this value\n",
        "      return value\n",
        "    # in case of rounding errors return last item in dict, shouldn't ever be reached\n",
        "  return list(prob_dict.keys())[-1]\n",
        "\n",
        "def run_gibbs(bayes_net, query, evidence, num_samples, burn_num, num_rums):\n",
        "    # runs gibbs sampling a specfied number of times to get a result that is an average over all of these runs\n",
        "    # input: bayes_net: bayes net object\n",
        "    # query: name of variable to we are trying to find out about\n",
        "    # evidence: dictionary for observed evidence\n",
        "   # num_sample: number of samples to do\n",
        "    # burn_num: number of intial samples to discard\n",
        "    # num_runs: number of times to run gibbs sampling\n",
        "    # output: prob distribution dictionary of query var that is average of all runs\n",
        "\n",
        "    # get the query node to be used to initialize average dict\n",
        "    query_node =  bayes_net.get_node(query)\n",
        "    # use query node to initialize average dict\n",
        "    avg_dist = {value: 0.0 for value in query_node.domain}\n",
        "\n",
        "    for i in range(num_rums):\n",
        "      # run gibbs sampling i times\n",
        "      result_dist = gibbs(bayes_net, query, evidence, num_samples, burn_num)\n",
        "      for value, prob in result_dist.items():\n",
        "        # add run to a running total\n",
        "        avg_dist[value] += prob\n",
        "    # normalize (effectively find the average)\n",
        "    final_average_dist = normalize(avg_dist)\n",
        "    return final_average_dist\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VARIABLE ELIMINATION - Min-degree hueristic\n",
        "\n",
        "def variable_elimination(bayes_net, query, evidence):\n",
        "  # PARAMETERS\n",
        "  # bayes_net: Bayesian network\n",
        "  # query: query variable\n",
        "  # evidence: dictionary for observed evidence\n",
        "  # RETURNS\n",
        "  # final_factor: a normalized Factor object\n",
        "\n",
        "  # Initialize list of factors\n",
        "  factors = []\n",
        "\n",
        "  # Loop over all nodes in bayesian network\n",
        "  for node_name, node in bayes_net.nodes.items():\n",
        "    # Store selected nodes CPT in factor\n",
        "    factor = node.cpt\n",
        "\n",
        "    # Loop through every piece of observed evidence\n",
        "    for evidence_variable, evidence_value in evidence.items():\n",
        "      # If factor includes that variable, restrict factor (keep only rows consistent with evidence)\n",
        "      if evidence_variable in factor.variable_names:\n",
        "        # New dictionary for probabilities that match evidence\n",
        "        new_probabilities = {}\n",
        "\n",
        "        # Loop through every row of factor's probability table\n",
        "        for assignment, probability in factor.probabilities.items():\n",
        "          # Map variable names to assignment values\n",
        "          row_dictionary = {}\n",
        "          for i in range(len(factor.variable_names)):\n",
        "            row_dictionary[factor.variable_names[i]] = assignment[i]\n",
        "\n",
        "          # Check evidence\n",
        "          if row_dictionary[evidence_variable] == evidence_value:\n",
        "\n",
        "            # Remove evidence variable from assignment\n",
        "            assignment_reduced = tuple(\n",
        "              row_dictionary[var] for var in factor.variable_names if var != evidence_variable\n",
        "            )\n",
        "            new_probabilities[assignment_reduced] = probability\n",
        "\n",
        "        # Build reduced factor\n",
        "        new_variables = [v for v in factor.variables if v.name != evidence_variable]\n",
        "        factor = Factor(new_variables, new_probabilities)\n",
        "\n",
        "    factors.append(factor)\n",
        "\n",
        "  # Min-degree elimination order\n",
        "  # Variables you are allowed to eliminate\n",
        "  elim_vars = [v for v in bayes_net.nodes.keys() if v not in query and v not in evidence]\n",
        "\n",
        "  # Count how many factors contain given variable\n",
        "  def factor_degree(var):\n",
        "    return sum(1 for f in factors if var in f.variable_names)\n",
        "\n",
        "  while elim_vars:\n",
        "\n",
        "    # Pick variable appearing in the FEWEST factors (min-degree)\n",
        "    variable = min(elim_vars, key=factor_degree)\n",
        "\n",
        "    # Collect factors containing this variable\n",
        "    factors_including_variable = [f for f in factors if variable in f.variable_names]\n",
        "\n",
        "    if len(factors_including_variable) > 0:\n",
        "      # Multiply them together\n",
        "      new_factor = factors_including_variable[0]\n",
        "      for f in factors_including_variable[1:]:\n",
        "        new_factor = Factor.multiply_factors(new_factor, f)\n",
        "\n",
        "      # Marginalize out the variable\n",
        "      new_factor = Factor.marginalize(new_factor, variable)\n",
        "\n",
        "      # Rebuild factor list\n",
        "      factors = [f for f in factors if f not in factors_including_variable]\n",
        "      factors.append(new_factor)\n",
        "\n",
        "    elim_vars.remove(variable)\n",
        "\n",
        "  # Multiply remaining factors\n",
        "  final_factor = factors[0]\n",
        "  for f in factors[1:]:\n",
        "    final_factor = Factor.multiply_factors(final_factor, f)\n",
        "\n",
        "  # Normalize\n",
        "  probability_total = sum(final_factor.probabilities.values())\n",
        "  normalized_probabilities = {\n",
        "    assignment: prob / probability_total\n",
        "    for assignment, prob in final_factor.probabilities.items()\n",
        "  }\n",
        "  final_factor = Factor(final_factor.variables, normalized_probabilities)\n",
        "\n",
        "  # Marginalize everything but the query variable\n",
        "  for var in list(final_factor.variable_names):\n",
        "    if var != query:\n",
        "      final_factor = Factor.marginalize(final_factor, var)\n",
        "\n",
        "  return final_factor\n"
      ],
      "metadata": {
        "id": "YOZ_RsPKkndD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znj0gJ9RSCik"
      },
      "outputs": [],
      "source": [
        "# parse evidence string\n",
        "def parse_evidence_string(evidence_str):\n",
        "    #converts the semicolon-separated evidence string into a dictionary\n",
        "    if not evidence_str:\n",
        "        return {}\n",
        "\n",
        "    evidence_dict = {}\n",
        "    pairs = evidence_str.strip().split(';')\n",
        "    for pair in pairs:\n",
        "        key, value = pair.split('=', 1)\n",
        "        evidence_dict[key.strip()] = value.strip()\n",
        "    return evidence_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkTEH7CIdzzP"
      },
      "outputs": [],
      "source": [
        "# FILE EXPORT\n",
        "import csv\n",
        "import os\n",
        "\n",
        "def file_export(results_dict, group_id, algorithm, network_name, evidence_level):\n",
        "\n",
        "    #Saves result probability distributions to CSV\n",
        "# input:results_dict: dictionary that holds results via key and value which is another dictionary that holds variables and their values\n",
        "# all other inputs are parameters\n",
        "    base_name = os.path.basename(network_name)\n",
        "\n",
        "    clean_network_name = os.path.splitext(base_name)[0]\n",
        "\n",
        "    filename = f\"{group_id}_{algorithm}_{clean_network_name}_{evidence_level}.csv\"\n",
        "\n",
        "    try:\n",
        "        # open file\n",
        "        with open(filename, 'w', newline='') as f:\n",
        "            #CSV writer object\n",
        "            writer = csv.writer(f)\n",
        "\n",
        "            # Loop each variable in the results dict\n",
        "            for variable_name, distribution_dict in results_dict.items():\n",
        "\n",
        "                # Get the domain values\n",
        "                domain_values = list(distribution_dict.keys())\n",
        "\n",
        "                #get probs\n",
        "                probabilities = list(distribution_dict.values())\n",
        "\n",
        "                # variable row\n",
        "                var_row = [variable_name] + domain_values\n",
        "                writer.writerow(var_row)\n",
        "\n",
        "               # prob row\n",
        "                value_row = [''] + probabilities\n",
        "                writer.writerow(value_row)\n",
        "        return filename\n",
        "\n",
        "    except IOError as e:\n",
        "        print(f\"Error: Could not write to file {filename}. {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo8UsM_Adusn"
      },
      "outputs": [],
      "source": [
        "# MAIN AUTOMATION// RUN EVERYTHING FROM HERE\n",
        "\n",
        "bayes_net = load_bif(NETWORK_NAME)\n",
        "\n",
        "evidence_dict = parse_evidence_string(EVIDENCE)\n",
        "\n",
        "query_vars = [name.strip() for name in REPORT.split(';')]\n",
        "\n",
        "final_results = {}\n",
        "\n",
        "if(ALGORITHM == \"gibbs\"):\n",
        "  # gibbs\n",
        "  burn_num = 100\n",
        "  samples_num = 1000\n",
        "  num_runs = 100\n",
        "  for query_name in query_vars:\n",
        "    result_dist = run_gibbs(bayes_net, query_name, evidence_dict, samples_num, burn_num, num_runs)\n",
        "    final_results[query_name] = result_dist\n",
        "  file_export(final_results, GROUP_ID, ALGORITHM, NETWORK_NAME, EVIDENCE_LEVEL)\n",
        "\n",
        "elif(ALGORITHM == \"ve\"):\n",
        "  # variable elimination\n",
        "  for query_name in query_vars:\n",
        "    result_factor = variable_elimination(bayes_net, query_name, evidence_dict)\n",
        "    # Convert Factor object to probability dictionary\n",
        "    result_dict = {}\n",
        "    for assignment, prob in result_factor.probabilities.items():\n",
        "      # Simplify tuple to its single value\n",
        "      if isinstance(assignment, tuple) and len(assignment) == 1:\n",
        "        assignment = assignment[0]\n",
        "      result_dict[str(assignment)] = prob\n",
        "    final_results[query_name] = result_dict\n",
        "  file_export(final_results, GROUP_ID, ALGORITHM, NETWORK_NAME, EVIDENCE_LEVEL)\n",
        "\n",
        "else:\n",
        "  print(\"unknown algorithm\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}